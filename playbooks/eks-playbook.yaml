---
# eks-playbook.yaml
- name: Setup EKS cluster and deploy containers
  hosts: localhost
  connection: local
  gather_facts: true
  vars_files:
    - vars.yml

  tasks:
    - name: Check if required Python packages are installed
      ansible.builtin.pip:
        name:
          - boto3
          - kubernetes
        state: present
      become: yes

    - name: Get AWS account ID
      amazon.aws.aws_caller_info:
        profile: "{{ aws.profile }}"
      register: aws_caller_info

    - name: Get VPC ID
      amazon.aws.ec2_vpc_net_info:
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        filters:
          "tag:Name": "{{ vpc.name }}"
      register: vpc_info

    - name: Get subnet information
      amazon.aws.ec2_vpc_subnet_info:
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        filters:
          vpc-id: "{{ vpc_info.vpcs[0].id }}"
      register: subnet_info

    - name: Create EKS cluster role
      community.aws.iam_role:
        name: "{{ eks.cluster_role_name }}"
        assume_role_policy_document: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "eks.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
              }
            ]
          }
        state: present
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
      register: cluster_role

    - name: Attach AmazonEKSClusterPolicy to cluster role
      ansible.builtin.command:
        cmd: "aws iam attach-role-policy --role-name {{ eks.cluster_role_name }} --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy --profile {{ aws.profile }} --region {{ aws.region }}"
      register: attach_cluster_policy
      changed_when: attach_cluster_policy.rc == 0
      failed_when: 
        - attach_cluster_policy.rc != 0 
        - '"EntityAlreadyExists" not in attach_cluster_policy.stderr'
      ignore_errors: yes

    - name: Wait for cluster role IAM propagation
      ansible.builtin.pause:
        seconds: 10

    - name: Create EKS cluster security group
      amazon.aws.ec2_security_group:
        name: "{{ eks.cluster_name }}-cluster-sg"
        description: Security group for EKS cluster
        vpc_id: "{{ vpc_info.vpcs[0].id }}"
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        rules:
          - proto: tcp
            ports: 
              - 443
            cidr_ip: 0.0.0.0/0
            rule_desc: Allow HTTPS from anywhere
          - proto: tcp
            from_port: 1025
            to_port: 65535
            cidr_ip: 0.0.0.0/0
            rule_desc: Allow return traffic for services
      register: eks_cluster_sg

    - name: Create EKS cluster
      community.aws.eks_cluster:
        name: "{{ eks.cluster_name }}"
        version: "{{ eks.kubernetes_version }}"
        role_arn: "arn:aws:iam::{{ aws_caller_info.account }}:role/{{ eks.cluster_role_name }}"
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        subnets: "{{ subnet_info.subnets | selectattr('tags.Name', 'match', '^private-subnet-.*') | map(attribute='id') | list }}"
        security_groups: 
          - "{{ eks_cluster_sg.group_id }}"
        state: present
        wait: yes
      register: eks_cluster

    # Configure kubectl immediately after cluster creation
    - name: Update kubeconfig with cluster info and credentials
      ansible.builtin.command:
        cmd: "aws eks update-kubeconfig --name {{ eks.cluster_name }} --region {{ aws.region }} --profile {{ aws.profile }}"
      register: kubeconfig_update
      changed_when: "'Updated context' in kubeconfig_update.stdout"

    - name: Create EKS node role
      community.aws.iam_role:
        name: "{{ eks.node_role_name }}"
        assume_role_policy_document: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "ec2.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
              }
            ]
          }
        state: present
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
      register: node_role

    - name: Attach policies to node role
      ansible.builtin.command:
        cmd: "aws iam attach-role-policy --role-name {{ eks.node_role_name }} --policy-arn {{ item }} --profile {{ aws.profile }} --region {{ aws.region }}"
      loop:
        - "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
        - "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
      register: attach_policy
      changed_when: attach_policy.rc == 0
      failed_when: 
        - attach_policy.rc != 0 
        - '"EntityAlreadyExists" not in attach_policy.stderr'
      ignore_errors: yes

    - name: Create instance profile for node role
      ansible.builtin.command:
        cmd: "aws iam create-instance-profile --instance-profile-name {{ eks.node_role_name }} --profile {{ aws.profile }} --region {{ aws.region }}"
      register: create_instance_profile
      changed_when: create_instance_profile.rc == 0
      failed_when: 
        - create_instance_profile.rc != 0
        - '"EntityAlreadyExists" not in create_instance_profile.stderr'
      ignore_errors: yes

    - name: Add role to instance profile
      ansible.builtin.command:
        cmd: "aws iam add-role-to-instance-profile --role-name {{ eks.node_role_name }} --instance-profile-name {{ eks.node_role_name }} --profile {{ aws.profile }} --region {{ aws.region }}"
      register: add_role_to_profile
      changed_when: add_role_to_profile.rc == 0
      failed_when:
        - add_role_to_profile.rc != 0
        - '"LimitExceeded" not in add_role_to_profile.stderr'
      ignore_errors: yes

    - name: Wait for IAM role propagation
      ansible.builtin.pause:
        seconds: 10

    - name: Create EKS node group
      community.aws.eks_nodegroup:
        name: "{{ eks.cluster_name }}-nodes"
        cluster_name: "{{ eks.cluster_name }}"
        node_role: "arn:aws:iam::{{ aws_caller_info.account }}:role/{{ eks.node_role_name }}"
        subnets: "{{ subnet_info.subnets | selectattr('tags.Name', 'match', '^private-subnet-.*') | map(attribute='id') | list }}"
        scaling_config:
          desired_size: 2
          max_size: 4
          min_size: 1
        disk_size: 20
        instance_types:
          - t3.medium
        tags:
          Environment: production
        state: present
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        wait: yes
      register: eks_nodegroup

    - name: Wait for nodegroup to be active
      ansible.builtin.pause:
        seconds: 60
      when: eks_nodegroup.changed

    - name: Wait for nodes to be ready
      ansible.builtin.command:
        cmd: "kubectl wait --for=condition=Ready nodes --all --timeout=300s"
      register: nodes_ready
      retries: 10
      delay: 30
      until: nodes_ready is not failed

    - name: Verify connection to cluster
      ansible.builtin.command:
        cmd: "kubectl get nodes"
      register: kubectl_test
      changed_when: false
      retries: 3
      delay: 5
      until: kubectl_test is not failed

    # Deploy Application
    - name: Create namespace
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ k8s.namespace }}"
        state: present

    - name: Apply Kubernetes Deployment
      kubernetes.core.k8s:
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: mag-site
            namespace: "{{ k8s.namespace }}"
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: mag-site
            template:
              metadata:
                labels:
                  app: mag-site
              spec:
                containers:
                - name: mag-site
                  image: "{{ aws_caller_info.account }}.dkr.ecr.{{ aws.region }}.amazonaws.com/{{ ecr.repository }}:latest"
                  ports:
                    - containerPort: 80
                  imagePullPolicy: Always
        state: present

    - name: Create LoadBalancer service
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: mag-site
            namespace: "{{ k8s.namespace }}"
          spec:
            type: LoadBalancer
            ports:
              - port: 80
                targetPort: 80
                protocol: TCP
            selector:
              app: mag-site

    - name: Wait for LoadBalancer External hostname
      ansible.builtin.command: kubectl get svc mag-site -n {{ k8s.namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
      register: loadbalancer_hostname
      until: loadbalancer_hostname.stdout != ""
      retries: 30
      delay: 10

    - name: Wait for DNS propagation
      ansible.builtin.command: >
        nslookup {{ loadbalancer_hostname.stdout }}
      register: dns_check
      until: dns_check.rc == 0
      retries: 30
      delay: 10
      ignore_errors: yes

    - name: Output LoadBalancer Hostname
      debug:
        msg: |
          Website is available at http://{{ loadbalancer_hostname.stdout }}
          Please allow a few minutes for DNS propagation and the service to be fully available.