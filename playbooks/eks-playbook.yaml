---
# eks-playbook.yaml
- name: Setup EKS cluster and deploy containers
  hosts: localhost
  connection: local
  gather_facts: true
  vars_files:
    - vars.yml

  tasks:
    - name: Check if required Python packages are installed
      ansible.builtin.pip:
        name:
          - boto3
          - kubernetes
        state: present
      become: yes

    - name: Get AWS account ID
      amazon.aws.aws_caller_info:
        profile: "{{ aws.profile }}"
      register: aws_caller_info

    - name: Get VPC ID
      amazon.aws.ec2_vpc_net_info:
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        filters:
          "tag:Name": "{{ vpc.name }}"
      register: vpc_info

    - name: Get subnet information
      amazon.aws.ec2_vpc_subnet_info:
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        filters:
          vpc-id: "{{ vpc_info.vpcs[0].id }}"
      register: subnet_info

    - name: Create EKS cluster role
      community.aws.iam_role:
        name: "{{ eks.cluster_role_name }}"
        assume_role_policy_document: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "eks.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
              }
            ]
          }
        state: present
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
      register: cluster_role

    - name: Attach AmazonEKSClusterPolicy to cluster role
      ansible.builtin.command:
        cmd: "aws iam attach-role-policy --role-name {{ eks.cluster_role_name }} --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy --profile {{ aws.profile }} --region {{ aws.region }}"
      register: attach_cluster_policy
      changed_when: attach_cluster_policy.rc == 0
      failed_when: 
        - attach_cluster_policy.rc != 0 
        - '"EntityAlreadyExists" not in attach_cluster_policy.stderr'
      ignore_errors: yes

    - name: Wait for cluster role IAM propagation
      ansible.builtin.pause:
        seconds: 10

    - name: Create EKS cluster security group
      amazon.aws.ec2_security_group:
        name: "{{ eks.cluster_name }}-cluster-sg"
        description: Security group for EKS cluster
        vpc_id: "{{ vpc_info.vpcs[0].id }}"
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        rules:
          - proto: tcp
            ports: 
              - 443
            cidr_ip: 0.0.0.0/0
            rule_desc: Allow HTTPS from anywhere
          - proto: tcp
            from_port: 1025
            to_port: 65535
            cidr_ip: 0.0.0.0/0
            rule_desc: Allow return traffic for services
      register: eks_cluster_sg

    - name: Create EKS cluster
      community.aws.eks_cluster:
        name: "{{ eks.cluster_name }}"
        version: "{{ eks.kubernetes_version }}"
        role_arn: "arn:aws:iam::{{ aws_caller_info.account }}:role/{{ eks.cluster_role_name }}"
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        subnets: "{{ subnet_info.subnets | selectattr('tags.Name', 'match', '^private-subnet-.*') | map(attribute='id') | list }}"
        security_groups: 
          - "{{ eks_cluster_sg.group_id }}"
        state: present
        wait: yes
      register: eks_cluster

    # Configure kubectl immediately after cluster creation
    - name: Update kubeconfig with cluster info and credentials
      ansible.builtin.command:
        cmd: "aws eks update-kubeconfig --name {{ eks.cluster_name }} --region {{ aws.region }} --profile {{ aws.profile }}"
      register: kubeconfig_update
      changed_when: "'Updated context' in kubeconfig_update.stdout"

    - name: Create EKS node role
      community.aws.iam_role:
        name: "{{ eks.node_role_name }}"
        assume_role_policy_document: |
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "ec2.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
              }
            ]
          }
        state: present
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
      register: node_role

    - name: Attach policies to node role
      ansible.builtin.command:
        cmd: "aws iam attach-role-policy --role-name {{ eks.node_role_name }} --policy-arn {{ item }} --profile {{ aws.profile }} --region {{ aws.region }}"
      loop:
        - "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
        - "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
      register: attach_policy
      changed_when: attach_policy.rc == 0
      failed_when: 
        - attach_policy.rc != 0 
        - '"EntityAlreadyExists" not in attach_policy.stderr'
      ignore_errors: yes

    - name: Create instance profile for node role
      ansible.builtin.command:
        cmd: "aws iam create-instance-profile --instance-profile-name {{ eks.node_role_name }} --profile {{ aws.profile }} --region {{ aws.region }}"
      register: create_instance_profile
      changed_when: create_instance_profile.rc == 0
      failed_when: 
        - create_instance_profile.rc != 0
        - '"EntityAlreadyExists" not in create_instance_profile.stderr'
      ignore_errors: yes

    - name: Add role to instance profile
      ansible.builtin.command:
        cmd: "aws iam add-role-to-instance-profile --role-name {{ eks.node_role_name }} --instance-profile-name {{ eks.node_role_name }} --profile {{ aws.profile }} --region {{ aws.region }}"
      register: add_role_to_profile
      changed_when: add_role_to_profile.rc == 0
      failed_when:
        - add_role_to_profile.rc != 0
        - '"LimitExceeded" not in add_role_to_profile.stderr'
      ignore_errors: yes

    - name: Wait for IAM role propagation
      ansible.builtin.pause:
        seconds: 10

    - name: Create EKS node group
      community.aws.eks_nodegroup:
        name: "{{ eks.cluster_name }}-nodes"
        cluster_name: "{{ eks.cluster_name }}"
        node_role: "arn:aws:iam::{{ aws_caller_info.account }}:role/{{ eks.node_role_name }}"
        subnets: "{{ subnet_info.subnets | selectattr('tags.Name', 'match', '^private-subnet-.*') | map(attribute='id') | list }}"
        scaling_config:
          desired_size: 2
          max_size: 4
          min_size: 1
        disk_size: 20
        instance_types:
          - t3.medium
        tags:
          Environment: production
        state: present
        region: "{{ aws.region }}"
        profile: "{{ aws.profile }}"
        wait: yes
      register: eks_nodegroup

    - name: Wait for nodegroup to be active
      ansible.builtin.pause:
        seconds: 60
      when: eks_nodegroup.changed

    - name: Wait for nodes to be ready
      ansible.builtin.command:
        cmd: "kubectl wait --for=condition=Ready nodes --all --timeout=300s"
      register: nodes_ready
      retries: 10
      delay: 30
      until: nodes_ready is not failed

    - name: Verify connection to cluster
      ansible.builtin.command:
        cmd: "kubectl get nodes"
      register: kubectl_test
      changed_when: false
      retries: 3
      delay: 5
      until: kubectl_test is not failed

    # Deploy Application
    - name: Create namespace
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ k8s.namespace }}"
        state: present

    - name: Apply Kubernetes Deployment
      kubernetes.core.k8s:
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: mag-site
            namespace: "{{ k8s.namespace }}"
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: mag-site
            template:
              metadata:
                labels:
                  app: mag-site
              spec:
                containers:
                - name: mag-site
                  image: "{{ aws_caller_info.account }}.dkr.ecr.{{ aws.region }}.amazonaws.com/{{ ecr.repository }}:latest"
                  ports:
                    - containerPort: 80
                  imagePullPolicy: Always
        state: present

    - name: Request ACM certificate
      ansible.builtin.command: >
        aws acm request-certificate
        --domain-name {{ domain.name }}
        --validation-method DNS
        --region {{ aws.region }}
        --profile {{ aws.profile }}
      register: certificate_request
      changed_when: true

    - name: Set certificate ARN
      set_fact:
        certificate_arn: "{{ (certificate_request.stdout | from_json).CertificateArn }}"

    - name: Get certificate validation info
      ansible.builtin.command: >
        aws acm describe-certificate
        --certificate-arn {{ certificate_arn }}
        --region {{ aws.region }}
        --profile {{ aws.profile }}
      register: certificate_info
      until: (certificate_info.stdout | from_json).Certificate.DomainValidationOptions[0].ResourceRecord is defined
      retries: 10
      delay: 5

    - name: Display DNS validation records
      debug:
        msg: |
          Please create the following DNS validation CNAME record in your DNS provider:
          Name: {{ (certificate_info.stdout | from_json).Certificate.DomainValidationOptions[0].ResourceRecord.Name }}
          Value: {{ (certificate_info.stdout | from_json).Certificate.DomainValidationOptions[0].ResourceRecord.Value }}

    - name: Wait for certificate validation
      ansible.builtin.command: >
        aws acm describe-certificate
        --certificate-arn {{ certificate_arn }}
        --region {{ aws.region }}
        --profile {{ aws.profile }}
      register: cert_status
      until: (cert_status.stdout | from_json).Certificate.Status == 'ISSUED'
      retries: 60
      delay: 30
      ignore_errors: true

    - name: Debug certificate_arn
      debug:
        var: certificate_arn

    - name: Create LoadBalancer service
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: mag-site
            namespace: "{{ k8s.namespace }}"
            annotations:
              service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "{{ certificate_arn }}"  # Changed from acm_certificate.certificate_arn
              service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
              service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
              service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
              service.beta.kubernetes.io/aws-load-balancer-actions.ssl-redirect: >
                {
                  "type": "redirect",
                  "redirectConfig": {
                    "protocol": "HTTPS",
                    "port": "443",
                    "statusCode": "HTTP_301"
                  }
                }
          spec:
            type: LoadBalancer
            ports:
              - port: 443
                targetPort: 80
                protocol: TCP
                name: https
              - port: 80
                targetPort: 80
                protocol: TCP
                name: http
            selector:
              app: mag-site

    - name: Wait for LoadBalancer External hostname
      ansible.builtin.command: kubectl get svc mag-site -n {{ k8s.namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
      register: loadbalancer_hostname
      until: loadbalancer_hostname.stdout != ""
      retries: 30
      delay: 10

    - name: Wait for LoadBalancer External hostname
      ansible.builtin.command: kubectl get svc mag-site -n {{ k8s.namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
      register: loadbalancer_hostname
      until: loadbalancer_hostname.stdout != ""
      retries: 30
      delay: 10

    - name: Wait for DNS propagation
      ansible.builtin.command: >
        nslookup {{ loadbalancer_hostname.stdout }}
      register: dns_check
      until: dns_check.rc == 0
      retries: 30
      delay: 10
      ignore_errors: yes

    - name: Output LoadBalancer Hostname and Next Steps
      debug:
        msg: |
          Website configuration complete:
          
          1. Create a CNAME record in your DNS provider:
             Name: {{ domain.name }}
             Type: CNAME
             Value: {{ loadbalancer_hostname.stdout }}
          
          2. The LoadBalancer is configured to:
             - Serve HTTPS on port 443
             - Automatically redirect HTTP to HTTPS
             - Use AWS ACM certificate: {{ certificate_arn }}
          
          3. Please allow a few minutes for:
             - DNS propagation
             - SSL certificate validation
             - LoadBalancer to become fully active